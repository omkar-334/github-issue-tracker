[
  {
    "repo": "matplotlib/matplotlib",
    "id": 296204085,
    "number": 10417,
    "title": "wxPython 4.0: backends not yet working w. Wayland / Ubuntu 17",
    "url": "https://github.com/matplotlib/matplotlib/issues/10417",
    "created_at": "2018-02-11T16:41:58Z",
    "user": "DietmarSchwertberger",
    "labels": [
      "GUI: wx",
      "keep"
    ],
    "has_linked_pr": false,
    "linked_pr_url": "",
    "archived": true
  },
  {
    "repo": "ray-project/ray",
    "id": 1768069437,
    "number": 36650,
    "title": "[vLLM/Serve] Create polished vLLM example on a Serve deployment",
    "url": "https://github.com/ray-project/ray/issues/36650",
    "created_at": "2023-06-21T17:17:15Z",
    "user": "cadedaniel",
    "labels": [
      "P1",
      "triage",
      "serve",
      "llm"
    ],
    "has_linked_pr": false,
    "linked_pr_url": "",
    "archived": true
  },
  {
    "repo": "ray-project/ray",
    "id": 2032730384,
    "number": 41728,
    "title": "Unable to run Batch inference with Multiple GPUs using LLM and Ray",
    "url": "https://github.com/ray-project/ray/issues/41728",
    "created_at": "2023-12-08T14:13:44Z",
    "user": "Roufa-mohammad-soulpage",
    "labels": [
      "triage",
      "@external-author-action-required",
      "data",
      "llm"
    ],
    "has_linked_pr": false,
    "linked_pr_url": "",
    "archived": true
  },
  {
    "repo": "ray-project/ray",
    "id": 2384413616,
    "number": 46360,
    "title": "[Serve] Expose internal VLLM metrics",
    "url": "https://github.com/ray-project/ray/issues/46360",
    "created_at": "2024-07-01T17:48:01Z",
    "user": "gilljon",
    "labels": [
      "enhancement",
      "dashboard",
      "triage",
      "observability",
      "llm"
    ],
    "has_linked_pr": false,
    "linked_pr_url": "",
    "archived": true
  }
]
