name: Monitor Issues

on:
  schedule:
    - cron: "*/10 * * * *"   # Every 10 minutes (incremental only)
  workflow_dispatch:
    inputs:
      full_sync:
        description: "Run FULL sync instead of incremental?"
        required: false
        type: boolean
        default: false

jobs:
  monitor:
    runs-on: ubuntu-latest

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Determine sync mode
        id: mode
        run: |
          # Manual dispatch with full_sync = true → full mode
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.full_sync }}" == "true" ]]; then
            echo "mode=full" >> $GITHUB_OUTPUT
          else
            echo "mode=incremental" >> $GITHUB_OUTPUT
          fi

      - name: Load last updated time
        id: last
        run: |
          if [ -f issues/index.json ]; then
            LAST_UPDATED=$(jq -r '.last_updated // empty' issues/index.json)
          fi
          echo "last_sync=${LAST_UPDATED:-1970-01-01T00:00:00Z}" >> $GITHUB_OUTPUT
          echo "Last sync: ${LAST_UPDATED:-1970}"

      - name: Read repository list
        id: repos
        run: |
          if [ ! -f repos.txt ]; then
            echo "repos.txt missing"
            exit 1
          fi
          REPOS=$(grep -v '^[[:space:]]*$' repos.txt | grep -v '^#')
          echo "repos<<EOF" >> $GITHUB_OUTPUT
          echo "$REPOS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Fetch issues (streaming mode)
        id: fetch
        env:
          TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p issues

          # temp file to store ALL issues collected this run
          ALL_TMP=$(mktemp)
          echo "[]" > "$ALL_TMP"

          MODE="${{ steps.mode.outputs.mode }}"

          if [ "$MODE" = "incremental" ]; then
            SINCE="${{ steps.last.outputs.last_sync }}"
            echo "Incremental sync since: $SINCE"
          else
            SINCE=""
            echo "FULL sync (manual dispatch)"
          fi

          # iterate repos
          while read repo; do
            [ -z "$repo" ] && continue

            echo "→ Fetching from $repo"

            PAGE=1
            PER_PAGE=100

            while true; do
              if [ -n "$SINCE" ]; then
                URL="https://api.github.com/repos/$repo/issues?state=open&per_page=$PER_PAGE&page=$PAGE&since=$SINCE"
              else
                URL="https://api.github.com/repos/$repo/issues?state=open&per_page=$PER_PAGE&page=$PAGE"
              fi

              RESP=$(curl -s -H "Authorization: token $TOKEN" "$URL")

              # verify JSON and check for errors
              if ! echo "$RESP" | jq empty >/dev/null 2>&1; then
                echo "Bad JSON for $repo page $PAGE — skipping"
                break
              fi
              
              # Check if response is an array (success) or error object
              if ! echo "$RESP" | jq -e 'type == "array"' >/dev/null 2>&1; then
                ERROR_MSG=$(echo "$RESP" | jq -r '.message // "Unknown error"' 2>/dev/null || echo "Invalid response format")
                echo "API error for $repo page $PAGE: $ERROR_MSG"
                # If rate limited, wait and retry
                if echo "$RESP" | jq -e '.message | contains("rate limit")' >/dev/null 2>&1; then
                  echo "Rate limited, waiting 60 seconds..."
                  sleep 60
                  continue
                fi
                break
              fi

              # extract non-PR issues only and add repo field
              # Filter: only objects (not strings), exclude PRs, add repo field
              echo "$RESP" | jq --arg repo "$repo" '
                if type == "array" then
                  [.[] | 
                    select(type == "object") | 
                    select(.pull_request == null) | 
                    . + {repo: $repo}]
                else
                  []
                end' > page.json
              
              # Verify page.json is valid
              if ! jq empty page.json >/dev/null 2>&1; then
                echo "Error processing issues for $repo page $PAGE"
                break
              fi
              
              COUNT=$(jq 'length' page.json)

              if [ "$COUNT" -eq 0 ]; then
                break
              fi

              # merge page.json → ALL_TMP (stream safe)
              jq -s '.[0] + .[1]' "$ALL_TMP" page.json > tmp.json
              mv tmp.json "$ALL_TMP"

              # if fewer than page size → last page
              if [ "$COUNT" -lt "$PER_PAGE" ]; then
                break
              fi

              PAGE=$((PAGE+1))
              sleep 1
            done

          done <<< "${{ steps.repos.outputs.repos }}"

          echo "all_tmp=$ALL_TMP" >> $GITHUB_OUTPUT

      - name: Process & store issues
        id: process
        run: |
          ALL_TMP="${{ steps.fetch.outputs.all_tmp }}"
          
          # Transform GitHub API format to our format (no PR checking)
          # Use file-based jq operations to avoid "Argument list too long" errors
          jq '[.[] | {
            repo: .repo,
            id: .id,
            number: .number,
            title: .title,
            url: .html_url,
            created_at: .created_at,
            user: .user.login,
            labels: [.labels[]?.name // empty],
            has_linked_pr: false,
            linked_pr_url: "",
            archived: false
          }]' "$ALL_TMP" > transformed.json
          
          COUNT=$(jq 'length' transformed.json)
          echo "Transformed $COUNT issues"

          # load existing (for dedupe) - use file-based operations to avoid argument list too long
          if [ -f issues/index.json ]; then
            jq '.issues // []' issues/index.json > existing.json
          else
            echo "[]" > existing.json
          fi

          # merge old + new → dedupe by .id (using files to avoid command-line length limits)
          jq -s 'add | unique_by(.id)' existing.json transformed.json > merged.json

          # archive threshold (1 year)
          CUTOFF=$(date -u -d '1 year ago' +%Y-%m-%d 2>/dev/null || date -u -v-1y +%Y-%m-%d 2>/dev/null || date -u +%Y-%m-%d)

          # Mark old issues as archived and split (using file-based operations)
          jq --arg cutoff "$CUTOFF" '[.[] | 
            if (.created_at | split("T")[0]) < $cutoff then 
              .archived = true 
            else 
              .archived = false 
            end]' merged.json > with_archive.json
          
          # Separate active and archived
          jq '[.[] | select(.archived == false)]' with_archive.json > active.json
          jq '[.[] | select(.archived == true)]' with_archive.json > archive_new.json

          # Merge with existing archive (if exists)
          if [ -f issues/archived.json ]; then
            jq -s 'add | unique_by(.id)' issues/archived.json archive_new.json > issues/archived.json
          else
            cp archive_new.json issues/archived.json
          fi

          # paginated files: one per YYYY-MM
          # Process issues one by one using file-based operations to avoid argument limits
          mkdir -p issues
          
          # Process each issue and add to appropriate YYYY-MM file
          jq -c '.[]' active.json | while IFS= read -r issue; do
            if [ -z "$issue" ]; then continue; fi
            
            # Extract year-month from created_at
            DATE=$(echo "$issue" | jq -r '.created_at // empty')
            if [ -z "$DATE" ] || [ "$DATE" = "null" ]; then continue; fi
            
            YM=$(echo "$DATE" | cut -d'T' -f1 | cut -d'-' -f1-2)
            if [ -z "$YM" ]; then continue; fi
            
            FILE="issues/$YM.json"
            
            # Initialize file if it doesn't exist
            if [ ! -f "$FILE" ]; then
              echo "[]" > "$FILE"
            fi
            
            # Add issue to file (using file-based jq to avoid argument limits)
            echo "$issue" | jq -s '.[0] + [.[1]] | unique_by(.id)' "$FILE" - > tmp_paginated.json
            mv tmp_paginated.json "$FILE"
          done

          # rebuild index.json (using file-based operation)
          COUNT=$(jq 'length' active.json)
          jq -n \
              --arg updated "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --slurpfile issues active.json \
              --argjson count "$COUNT" \
              '{issues: $issues[0], last_updated: $updated, total_count: $count}' > issues/index.json

          echo "active_count=$COUNT" >> $GITHUB_OUTPUT
          echo "Processed $COUNT active issues"
          
          # Cleanup temporary files
          rm -f transformed.json existing.json merged.json with_archive.json active.json archive_new.json page.json tmp.json "$ALL_TMP"

      - name: Commit & push
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          git add issues/

          if git diff --cached --quiet; then
            echo "No changes."
            exit 0
          fi

          git commit -m "Issue monitor update"
          git push
