name: Monitor Issues

on:
  schedule:
    - cron: "*/10 * * * *"   # Every 10 minutes (incremental only)
  workflow_dispatch:
    inputs:
      full_sync:
        description: "Run FULL sync instead of incremental?"
        required: false
        type: boolean
        default: false

jobs:
  monitor:
    runs-on: ubuntu-latest

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Determine sync mode
        id: mode
        run: |
          # Manual dispatch with full_sync = true → full mode
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.full_sync }}" == "true" ]]; then
            echo "mode=full" >> $GITHUB_OUTPUT
          else
            echo "mode=incremental" >> $GITHUB_OUTPUT
          fi

      - name: Load last updated time
        id: last
        run: |
          if [ -f issues/index.json ]; then
            LAST_UPDATED=$(jq -r '.last_updated // empty' issues/index.json)
          fi
          echo "last_sync=${LAST_UPDATED:-1970-01-01T00:00:00Z}" >> $GITHUB_OUTPUT
          echo "Last sync: ${LAST_UPDATED:-1970}"

      - name: Read repository list
        id: repos
        run: |
          if [ ! -f repos.txt ]; then
            echo "repos.txt missing"
            exit 1
          fi
          REPOS=$(grep -v '^[[:space:]]*$' repos.txt | grep -v '^#')
          echo "repos<<EOF" >> $GITHUB_OUTPUT
          echo "$REPOS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Fetch issues (streaming mode)
        id: fetch
        env:
          TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p issues

          # temp file to store ALL issues collected this run
          ALL_TMP=$(mktemp)
          echo "[]" > "$ALL_TMP"

          MODE="${{ steps.mode.outputs.mode }}"

          if [ "$MODE" = "incremental" ]; then
            SINCE="${{ steps.last.outputs.last_sync }}"
            echo "Incremental sync since: $SINCE"
          else
            SINCE=""
            echo "FULL sync (manual dispatch)"
          fi

          # iterate repos
          while read repo; do
            [ -z "$repo" ] && continue

            echo "→ Fetching from $repo"

            PAGE=1
            PER_PAGE=100

            while true; do
              if [ -n "$SINCE" ]; then
                URL="https://api.github.com/repos/$repo/issues?state=open&per_page=$PER_PAGE&page=$PAGE&since=$SINCE"
              else
                URL="https://api.github.com/repos/$repo/issues?state=open&per_page=$PER_PAGE&page=$PAGE"
              fi

              RESP=$(curl -s -H "Authorization: token $TOKEN" "$URL")

              # verify JSON
              if ! echo "$RESP" | jq empty >/dev/null 2>&1; then
                echo "Bad JSON for $repo page $PAGE — skipping"
                break
              fi

              # extract non-PR issues only and add repo field
              echo "$RESP" | jq --arg repo "$repo" '[.[] | select(.pull_request | not) | . + {repo: $repo}]' > page.json
              COUNT=$(jq 'length' page.json)

              if [ "$COUNT" -eq 0 ]; then
                break
              fi

              # merge page.json → ALL_TMP (stream safe)
              jq -s '.[0] + .[1]' "$ALL_TMP" page.json > tmp.json
              mv tmp.json "$ALL_TMP"

              # if fewer than page size → last page
              if [ "$COUNT" -lt "$PER_PAGE" ]; then
                break
              fi

              PAGE=$((PAGE+1))
              sleep 1
            done

          done <<< "${{ steps.repos.outputs.repos }}"

          echo "all_tmp=$ALL_TMP" >> $GITHUB_OUTPUT

      - name: Process & store issues
        id: process
        run: |
          ALL_TMP="${{ steps.fetch.outputs.all_tmp }}"
          TOKEN="${{ secrets.GITHUB_TOKEN }}"
          
          # Transform GitHub API format to our format and detect PR links
          # Process each issue to check for linked PRs
          TRANSFORMED=$(mktemp)
          echo "[]" > "$TRANSFORMED"
          
          jq -c '.[]' "$ALL_TMP" | while IFS= read -r issue; do
            if [ -z "$issue" ]; then continue; fi
            
            REPO=$(echo "$issue" | jq -r '.repo // empty')
            ISSUE_NUM=$(echo "$issue" | jq -r '.number // empty')
            BODY=$(echo "$issue" | jq -r '.body // ""')
            
            # Check issue body for PR references
            HAS_PR=false
            PR_URL=""
            
            # Check body for PR patterns
            if [ -n "$BODY" ] && [ "$BODY" != "null" ]; then
              if echo "$BODY" | grep -qiE "(closes|fixes|resolves).*#\d+|github\.com.*pull/\d+"; then
                HAS_PR=true
                # Try to extract PR URL from body
                PR_URL=$(echo "$BODY" | grep -oiE "https://github\.com/[^/]+/[^/]+/pull/\d+" | head -1 || echo "")
              fi
            fi
            
            # Check issue timeline for linked PRs (if not found in body)
            if [ "$HAS_PR" = "false" ] && [ -n "$REPO" ] && [ -n "$ISSUE_NUM" ]; then
              TIMELINE=$(curl -s -H "Authorization: token $TOKEN" \
                "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM/timeline" 2>/dev/null || echo "[]")
              
              if echo "$TIMELINE" | jq empty >/dev/null 2>&1; then
                PR_REFS=$(echo "$TIMELINE" | jq '[.[] | select(.source?.issue?.pull_request != null)]' 2>/dev/null || echo "[]")
                if [ "$(echo "$PR_REFS" | jq 'length' 2>/dev/null)" -gt 0 ]; then
                  HAS_PR=true
                  PR_URL=$(echo "$PR_REFS" | jq -r '.[0].source.issue.html_url // ""' 2>/dev/null || echo "")
                fi
              fi
            fi
            
            # Transform to our format
            TRANSFORMED_ISSUE=$(echo "$issue" | jq --argjson has_pr "$HAS_PR" --arg pr_url "$PR_URL" '{
              repo: .repo,
              id: .id,
              number: .number,
              title: .title,
              url: .html_url,
              created_at: .created_at,
              user: .user.login,
              labels: [.labels[]?.name // empty],
              has_linked_pr: $has_pr,
              linked_pr_url: $pr_url,
              archived: false
            }')
            
            # Add to transformed array
            jq --argjson new "$TRANSFORMED_ISSUE" '. + [$new]' "$TRANSFORMED" > tmp_transformed.json
            mv tmp_transformed.json "$TRANSFORMED"
          done
          
          # Final transformed file
          mv "$TRANSFORMED" transformed.json

          # load existing (for dedupe)
          if [ -f issues/index.json ]; then
            EXIST=$(jq '.issues // []' issues/index.json)
          else
            EXIST="[]"
          fi

          # merge old + new → dedupe by .id
          MERGED=$(jq -s 'add | unique_by(.id)' <(echo "$EXIST") transformed.json)

          # archive threshold (1 year)
          CUTOFF=$(date -u -d '1 year ago' +%Y-%m-%d 2>/dev/null || date -u -v-1y +%Y-%m-%d 2>/dev/null || date -u +%Y-%m-%d)

          # Mark old issues as archived and split
          ACTIVE=$(echo "$MERGED" | jq --arg cutoff "$CUTOFF" '[.[] | 
            if (.created_at | split("T")[0]) < $cutoff then 
              .archived = true 
            else 
              .archived = false 
            end]')
          
          # Separate active and archived
          ACTIVE_ONLY=$(echo "$ACTIVE" | jq '[.[] | select(.archived == false)]')
          ARCHIVE_ONLY=$(echo "$ACTIVE" | jq '[.[] | select(.archived == true)]')

          # Merge with existing archive
          if [ -f issues/archived.json ]; then
            EXISTING_ARCHIVE=$(jq '.' issues/archived.json 2>/dev/null || echo "[]")
            ARCHIVE_ONLY=$(jq -s 'add | unique_by(.id)' <(echo "$EXISTING_ARCHIVE") <(echo "$ARCHIVE_ONLY"))
          fi
          
          # save archive
          echo "$ARCHIVE_ONLY" | jq '.' > issues/archived.json

          # paginated files: one per YYYY-MM
          # Use jq to process each issue safely (handles JSON with newlines)
          echo "$ACTIVE_ONLY" | jq -c '.[]' | while IFS= read -r issue; do
            if [ -z "$issue" ]; then continue; fi
            DATE=$(echo "$issue" | jq -r '.created_at // empty')
            if [ -z "$DATE" ]; then continue; fi
            YM=$(echo "$DATE" | cut -d'T' -f1 | cut -d'-' -f1-2)
            if [ -z "$YM" ]; then continue; fi
            mkdir -p issues
            FILE="issues/$YM.json"
            if [ ! -f "$FILE" ]; then echo "[]" > "$FILE"; fi
            # Merge safely using jq
            jq --argjson new "$issue" '. + [$new] | unique_by(.id)' "$FILE" > tmp.json && mv tmp.json "$FILE"
          done

          # rebuild index.json
          COUNT=$(echo "$ACTIVE_ONLY" | jq 'length')
          INDEX=$(jq -n \
              --arg updated "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --argjson issues "$ACTIVE_ONLY" \
              --argjson count "$COUNT" \
              '{issues: $issues, last_updated: $updated, total_count: $count}')

          echo "$INDEX" > issues/index.json

          echo "active_count=$COUNT" >> $GITHUB_OUTPUT
          
          # Cleanup
          rm -f transformed.json page.json tmp.json "$ALL_TMP"

      - name: Commit & push
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          git add issues/

          if git diff --cached --quiet; then
            echo "No changes."
            exit 0
          fi

          git commit -m "Issue monitor update"
          git push
